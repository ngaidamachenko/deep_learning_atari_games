{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the DQN Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(7*7*64, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)  #Convert from [batch_size, 1, channels, height, width] to [batch_size, channels, height, width]\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  #Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the Space Invaders environment\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "#Number of possible actions\n",
    "action_dim = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize primary and target DQN networks\n",
    "dqn = DQN(action_dim)\n",
    "target_dqn = DQN(action_dim)\n",
    "#Copy weights from the primary network to the target network\n",
    "target_dqn.load_state_dict(dqn.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "\n",
    "#set up the Adam optimizer with a learning rate of 0.00025\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=0.00025)\n",
    "#define the loss function \n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "#Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "#Initial epsilon (exploration rate)\n",
    "epsilon = 1.0\n",
    "#Epsilon decay rate after each episode\n",
    "epsilon_decay = 0.995\n",
    "#Exploration progression rate\n",
    "epsilon_min = 0.1\n",
    "#Memory buffer to store experiences\n",
    "memory = []\n",
    "#Maximum number of experiences to keep in memory\n",
    "max_memory = 10000\n",
    "batch_size = 32\n",
    "episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess the image\n",
    "def preprocess(image):\n",
    "    #Convert image to grayscale\n",
    "    image = Image.fromarray(image).convert('L')\n",
    "    image = image.resize((84, 84))\n",
    "    #Convert to numpy array and normalize the pixel values\n",
    "    image = np.array(image) / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to select an action using epsilon-greedy policy\n",
    "def select_action(state, epsilon):\n",
    "    # Random action with exploration probability\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        #Else, rely on exploitation (experience)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            return dqn(state).argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train the DQN using mini-batch from the memory buffer\n",
    "def train():\n",
    "    # Skip training if there are not enough experiences\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    #Sample a mini-batch of experiences\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    #Unpack the batch\n",
    "    state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
    "    \n",
    "    #Convert batch to PyTorch tensors\n",
    "    state_batch = torch.FloatTensor(state_batch).to(device)\n",
    "    action_batch = torch.LongTensor(action_batch).unsqueeze(1).to(device)\n",
    "    reward_batch = torch.FloatTensor(reward_batch).to(device)\n",
    "    next_state_batch = torch.FloatTensor(next_state_batch).to(device)\n",
    "    done_batch = torch.FloatTensor(done_batch).to(device)\n",
    "    \n",
    "    #Calculate Q-values for the current state-action pairs\n",
    "    q_values = dqn(state_batch).gather(1, action_batch)\n",
    "    #Calculate Q-values for the next state using the target network\n",
    "    next_q_values = target_dqn(next_state_batch).max(1)[0].unsqueeze(1)\n",
    "    \n",
    "    #Compute the target Q-value using the Bellman equation\n",
    "    expected_q_values = reward_batch + gamma * next_q_values * (1 - done_batch)\n",
    "\n",
    "    #Calculate the loss between the current Q-value and the target Q-value\n",
    "    loss = loss_fn(q_values, expected_q_values.detach())\n",
    "    #Zero the gradients for the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    #Backpropagate the loss\n",
    "    loss.backward()\n",
    "    #Update the network weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:939: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 125.0, Epsilon: 0.9950\n",
      "Episode: 1, Total Reward: 260.0, Epsilon: 0.9900\n",
      "Episode: 2, Total Reward: 125.0, Epsilon: 0.9851\n",
      "Episode: 3, Total Reward: 105.0, Epsilon: 0.9801\n",
      "Episode: 4, Total Reward: 210.0, Epsilon: 0.9752\n",
      "Episode: 5, Total Reward: 290.0, Epsilon: 0.9704\n",
      "Episode: 6, Total Reward: 180.0, Epsilon: 0.9655\n",
      "Episode: 7, Total Reward: 50.0, Epsilon: 0.9607\n",
      "Episode: 8, Total Reward: 270.0, Epsilon: 0.9559\n",
      "Episode: 9, Total Reward: 240.0, Epsilon: 0.9511\n",
      "Episode: 10, Total Reward: 80.0, Epsilon: 0.9464\n",
      "Episode: 11, Total Reward: 180.0, Epsilon: 0.9416\n",
      "Episode: 12, Total Reward: 285.0, Epsilon: 0.9369\n",
      "Episode: 13, Total Reward: 45.0, Epsilon: 0.9322\n",
      "Episode: 14, Total Reward: 35.0, Epsilon: 0.9276\n",
      "Episode: 15, Total Reward: 505.0, Epsilon: 0.9229\n",
      "Episode: 16, Total Reward: 135.0, Epsilon: 0.9183\n",
      "Episode: 17, Total Reward: 135.0, Epsilon: 0.9137\n",
      "Episode: 18, Total Reward: 165.0, Epsilon: 0.9092\n",
      "Episode: 19, Total Reward: 155.0, Epsilon: 0.9046\n",
      "Episode: 20, Total Reward: 90.0, Epsilon: 0.9001\n",
      "Episode: 21, Total Reward: 215.0, Epsilon: 0.8956\n",
      "Episode: 22, Total Reward: 160.0, Epsilon: 0.8911\n",
      "Episode: 23, Total Reward: 300.0, Epsilon: 0.8867\n",
      "Episode: 24, Total Reward: 30.0, Epsilon: 0.8822\n",
      "Episode: 25, Total Reward: 70.0, Epsilon: 0.8778\n",
      "Episode: 26, Total Reward: 10.0, Epsilon: 0.8734\n",
      "Episode: 27, Total Reward: 105.0, Epsilon: 0.8691\n",
      "Episode: 28, Total Reward: 535.0, Epsilon: 0.8647\n",
      "Episode: 29, Total Reward: 395.0, Epsilon: 0.8604\n",
      "Episode: 30, Total Reward: 60.0, Epsilon: 0.8561\n",
      "Episode: 31, Total Reward: 415.0, Epsilon: 0.8518\n",
      "Episode: 32, Total Reward: 415.0, Epsilon: 0.8475\n",
      "Episode: 33, Total Reward: 120.0, Epsilon: 0.8433\n",
      "Episode: 34, Total Reward: 75.0, Epsilon: 0.8391\n",
      "Episode: 35, Total Reward: 100.0, Epsilon: 0.8349\n",
      "Episode: 36, Total Reward: 95.0, Epsilon: 0.8307\n",
      "Episode: 37, Total Reward: 105.0, Epsilon: 0.8266\n",
      "Episode: 38, Total Reward: 135.0, Epsilon: 0.8224\n",
      "Episode: 39, Total Reward: 55.0, Epsilon: 0.8183\n",
      "Episode: 40, Total Reward: 190.0, Epsilon: 0.8142\n",
      "Episode: 41, Total Reward: 180.0, Epsilon: 0.8102\n",
      "Episode: 42, Total Reward: 55.0, Epsilon: 0.8061\n",
      "Episode: 43, Total Reward: 425.0, Epsilon: 0.8021\n",
      "Episode: 44, Total Reward: 80.0, Epsilon: 0.7981\n",
      "Episode: 45, Total Reward: 75.0, Epsilon: 0.7941\n",
      "Episode: 46, Total Reward: 85.0, Epsilon: 0.7901\n",
      "Episode: 47, Total Reward: 50.0, Epsilon: 0.7862\n",
      "Episode: 48, Total Reward: 425.0, Epsilon: 0.7822\n",
      "Episode: 49, Total Reward: 35.0, Epsilon: 0.7783\n",
      "Episode: 50, Total Reward: 125.0, Epsilon: 0.7744\n",
      "Episode: 51, Total Reward: 410.0, Epsilon: 0.7705\n",
      "Episode: 52, Total Reward: 155.0, Epsilon: 0.7667\n",
      "Episode: 53, Total Reward: 290.0, Epsilon: 0.7629\n",
      "Episode: 54, Total Reward: 230.0, Epsilon: 0.7590\n",
      "Episode: 55, Total Reward: 275.0, Epsilon: 0.7553\n",
      "Episode: 56, Total Reward: 20.0, Epsilon: 0.7515\n",
      "Episode: 57, Total Reward: 80.0, Epsilon: 0.7477\n",
      "Episode: 58, Total Reward: 105.0, Epsilon: 0.7440\n",
      "Episode: 59, Total Reward: 155.0, Epsilon: 0.7403\n",
      "Episode: 60, Total Reward: 75.0, Epsilon: 0.7366\n",
      "Episode: 61, Total Reward: 5.0, Epsilon: 0.7329\n",
      "Episode: 62, Total Reward: 125.0, Epsilon: 0.7292\n",
      "Episode: 63, Total Reward: 130.0, Epsilon: 0.7256\n",
      "Episode: 64, Total Reward: 110.0, Epsilon: 0.7219\n",
      "Episode: 65, Total Reward: 95.0, Epsilon: 0.7183\n",
      "Episode: 66, Total Reward: 220.0, Epsilon: 0.7147\n",
      "Episode: 67, Total Reward: 105.0, Epsilon: 0.7112\n",
      "Episode: 68, Total Reward: 120.0, Epsilon: 0.7076\n",
      "Episode: 69, Total Reward: 90.0, Epsilon: 0.7041\n",
      "Episode: 70, Total Reward: 60.0, Epsilon: 0.7005\n",
      "Episode: 71, Total Reward: 215.0, Epsilon: 0.6970\n",
      "Episode: 72, Total Reward: 120.0, Epsilon: 0.6936\n",
      "Episode: 73, Total Reward: 305.0, Epsilon: 0.6901\n",
      "Episode: 74, Total Reward: 180.0, Epsilon: 0.6866\n",
      "Episode: 75, Total Reward: 90.0, Epsilon: 0.6832\n",
      "Episode: 76, Total Reward: 180.0, Epsilon: 0.6798\n",
      "Episode: 77, Total Reward: 105.0, Epsilon: 0.6764\n",
      "Episode: 78, Total Reward: 160.0, Epsilon: 0.6730\n",
      "Episode: 79, Total Reward: 65.0, Epsilon: 0.6696\n",
      "Episode: 80, Total Reward: 130.0, Epsilon: 0.6663\n",
      "Episode: 81, Total Reward: 105.0, Epsilon: 0.6630\n",
      "Episode: 82, Total Reward: 195.0, Epsilon: 0.6597\n",
      "Episode: 83, Total Reward: 465.0, Epsilon: 0.6564\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 181\u001b[0m\n\u001b[0;32m    178\u001b[0m     memory\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Train the DQN on a mini-batch of experiences\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Update the current state buffer\u001b[39;00m\n\u001b[0;32m    184\u001b[0m state_buffer \u001b[38;5;241m=\u001b[39m next_state_buffer\n",
      "Cell \u001b[1;32mIn[3], line 101\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(action_batch)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    100\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(reward_batch)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 101\u001b[0m next_state_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    102\u001b[0m done_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(done_batch)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Calculate Q-values for the current state-action pairs\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#GPU Boost\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dqn.to(device)\n",
    "target_dqn.to(device)\n",
    "\n",
    "state_buffer = []\n",
    "\n",
    "#Main training loop\n",
    "for episode in range(episodes):\n",
    "    reset_result = env.reset()\n",
    "    #Handle reset return depending on its type (tuple or single value)\n",
    "    if isinstance(reset_result, tuple):\n",
    "        state = reset_result[0]\n",
    "    else:\n",
    "        state = reset_result\n",
    "    \n",
    "    state_buffer.clear()\n",
    "    processed_frame = preprocess(state)\n",
    "    state_buffer = [processed_frame] * 4  #Initialize with 4 frames\n",
    "\n",
    "    total_reward = 0 \n",
    "    \n",
    "    for t in range(10000):  # number of steps per episode\n",
    "        #Stack the state buffer and reshape it for the model\n",
    "        stacked_state = np.stack(state_buffer, axis=0)\n",
    "        stacked_state = np.expand_dims(stacked_state, axis=0)  # Add batch dimension\n",
    "        \n",
    "        #Remove the extra dimension\n",
    "        stacked_state = stacked_state.squeeze(0)\n",
    "\n",
    "        #Select an action based on the current state and epsilon\n",
    "        action = select_action(stacked_state, epsilon)\n",
    "        \n",
    "        #Perform the action and observe the next state, reward, and done flag\n",
    "        next_step_result = env.step(action)\n",
    "        if isinstance(next_step_result, tuple):\n",
    "            if len(next_step_result) == 5:\n",
    "                next_state, reward, done, truncated, info = next_step_result\n",
    "                done = done or truncated\n",
    "            else:\n",
    "                next_state, reward, done, info = next_step_result\n",
    "        else:\n",
    "            next_state, reward, done, info = next_step_result\n",
    "\n",
    "        processed_next_frame = preprocess(next_state)\n",
    "        #Update the state buffer with the new frame\n",
    "        next_state_buffer = state_buffer[1:] + [processed_next_frame]\n",
    "        \n",
    "        #Store the experience in memory\n",
    "        memory.append((np.stack(state_buffer, axis=0), action, reward, np.stack(next_state_buffer, axis=0), done))\n",
    "        #Remove the oldest experience if memory exceeds the maximum size\n",
    "        if len(memory) > max_memory:\n",
    "            memory.pop(0)\n",
    "        \n",
    "        train()\n",
    "        \n",
    "        #Update the current state buffer\n",
    "        state_buffer = next_state_buffer\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    #Decay epsilon to reduce exploration over time\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    #Update the target network with the primary network weights every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        target_dqn.load_state_dict(dqn.state_dict())\n",
    "    \n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}\")\n",
    "env\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
